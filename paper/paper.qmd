---
title: "Factors influencing Total Income in the United States"
subtitle: "Analyzing Factors influencing Total Income in the United States of America from 2019 to 2022"
author: 
  - Wen Han Zhao
thanks: "Code and data are available at: [https://github.com/younazhao/Income-Influencing-Factors.git](https://github.com/younazhao/Income-Influencing-Factors.git)."
date: 1 December 2024
date-format: long
abstract: "This paper develops a Bayes generalized linear model to predict the Total Income of United States of America through a survey conducte from 2019 to 2022. By analyzing factors such as Age, Sex, Marital Status, Education Level, Labour Force, Work Status and Occupation Category. By integrating these factors, we are able to see several trends and differences in each of those variables in order to best predict the Total Income in each individuals. Our results show that Male workers with higher education level at a managerial position will have higher Income. This model offers a comprehensive tool for understanding the different categories in each individual group and improving the accuracy of Total Income predictions."
format: pdf
number-sections: true
bibliography: references.bib
toc: true
---

```{r}
#| include: false
#| warning: false
#| message: false
#install.packages("kableExtra")
library(tidyverse)
library(arrow)
library(dplyr)
library(tibble)
library(knitr)
library(kableExtra)
library(rstanarm)
```

```{r}
#| include: false
#| warning: false
#| message: false

data <- read_parquet(here::here("data/02-analysis_data/analysis_data.parquet"))
```


# Introduction

Income inequality is a central concern in economic research, reflecting disparities in access to opportunities, resources, and outcomes. In the United States, total income is influenced by a variety of factors, including personal, professional, and demographic characteristics. This paper explores the relationship between total income and predictors such as marital status, sex, educational level, labor force participation, and occupation. By employing statistical modeling techniques, we aim to uncover the extent to which these variables shape income levels and contribute to broader income disparities.

The estimand of this study is the expected total income for individuals based on their social, personal and occupational characteristics. Specifically, the analysis seeks to quantify how factors such as marital status, sex, educational attainment, participation in the labor force, and occupational category predict variations in total income across the U.S. population. This includes identifying both independent and among these variables.

Preliminary results indicate that educational attainment is one of the strongest predictors of total income, with higher levels of education yielding significantly greater earnings. Marital status also plays a notable role, with married individuals generally earning more than their single counterparts. Gender disparities in income persist, with men earning more on average than women across most occupational categories. Labor force participation and occupation further highlight critical differences, as full-time workers and those in higher-skilled professions consistently report higher incomes.

Understanding the drivers of income disparities is essential for designing policies that promote economic equity and mobility. By identifying how marital status, gender, education, labor force participation, and occupation contribute to income levels, this research offers actionable insights for policymakers, employers, and educators. Addressing these disparities not only fosters greater fairness but also enhances the overall economic productivity and social cohesion of the United States.

To do this, total income was analyzed in Section 2 @sec-data. The prediction from the model has show accuracy in lower total income group. The model has shown difficulties in predicting higher total income. Section 3 @sec-model has further explanation on the Bayesian Generalized Linear Model. Section 4 @sec-results show the coefficient beta from the model. Section 5 @sec-discussion focus on the weakness of this study and further possible improvements. Lastly, Section 6 @sec-appendix contains methodology to perform a survey to collect data for Total Income.

The data gathering and analysis is done in R [@citeR] with the following packages: @tidyverse, @knitr, @arrow, @dplyr, @tibble, @kableExtra, and @rstanarm. 

# Data {#sec-data}

## Overview {#sec-data-overview}

The Survey of Consumer Finances (SCF), conducted in 2022 covering the period 1989 to 2022 by the Board of Governors of the Federal Reserve System of the United States [@scf2022], provides a comprehensive snapshot of household financial conditions across the nation. This dataset includes detailed information related to total income, capturing key predictors such as sex, education level, occupation category, salary income, and income from other sources. By offering rich, granular data, the SCF serves as an invaluable resource for analyzing the factors influencing income distribution and economic disparities within the United States.

## Measurement {#sec-data-measurement}

The survey contained a panel element over 2 periods. Respondents to the 1983 survey were re-interviewed in 1986 and 1989. Respondents to the 2007 survey were re-interviewed in 2009. In order to ensure the representativeness of the study, respondents are selected randomly in order to attempt families from all economic strata. 

## Outcome variables{#sec-data-outcome}

The histogram represents the distribution of our outcome variables total income among survey participants. From the graph, we can see that the graph shows slightly right skewed but mostly symmetric by following a bell shape curve. It shows a mean centered 11 to 12, indicating that most individuals in the survey have incomes within this range. The spread of the graph shows approximately from 8 to 16 of the log income and contains only a few extreme outliers at the tails. Overall, the distribution of log income, the outcome variable, follows a normal distribution. 

```{r}
#| label: fig-hist
#| fig-cap: Distribution of Total Income 
#| echo: false
data$log_income <- log(data$income)
ggplot(data, aes(x = log_income)) +
  geom_histogram(binwidth = 0.5, fill = "blue", color = "black", alpha = 0.7) +
  labs(x = "Log Income", y = "Frequency") +
  theme_minimal()
```
\pagebreak

Quantile-quantile plot is included to compare the log-transformed income data' distribution to a theoretical normal distribution. The data should expect to follow closely to the blue dashed line to indicate a normal distribution. From the graph, we can see that the middle of the plot lies closely to the line which indicate that the data follows a normal distribution. The right tail have various points deviate above the line, which suggest a heavier right tail. The deviation form the right tail is expected, since higher income are more random and harder to predict. 

```{r}
#| label: fig-qq
#| fig-cap: Comparison of Income Distribution using QQ Plot
#| echo: false
#| warning: false
#| message: false

ggplot(data = data, aes(sample = log_income)) +
  geom_qq(color = "black", size = 1, alpha = 0.7) + # Adds blue points with some transparency
  geom_qq_line(color = "blue", linetype = "dashed", size = 1) + # Adds a red dashed reference line
  theme_minimal() +
  labs(
    x = "Theoretical Quantiles",
    y = "Sample Quantiles"
  )

```
\pagebreak

## Predictor variables{#sec-data-predictor}

Predictors variables selected are Age, Sex, Marital Status, Education Level, Labour Force, Work Status, and Occupation category.

In @fig-pred-age, the graph shows a scatter plot with Age on the x-axis and Log Income on the y-axis, with individual data points represented as black circles. A red regression line is overlaid, indicating the linear relationship between age and log-transformed income. The plot suggests a positive correlation between age and income, where income increases as age increases. However, the relationship appears relatively weak, as the data points are widely spread around the regression line, indicating substantial variability in income across different ages. Despite this, the general upward trend is clear, showing that older individuals tend to have higher log-transformed income. Therefore, we decided to not use this variable in our rating. 

```{r}
#| label: fig-pred-age
#| fig-cap: Log Income by Age
#| echo: false
#| warning: false
#| message: false

plot(data$Age, data$log_income, type = "p", 
     xlab = "Age", ylab = "Log Income")
abline(lm(log_income ~ Age, data = data), col = "red", lwd = 2)

```
\pagebreak

In @fig-pred-sex, the graph is a boxplot comparing log-transformed income (Log Income) between females and males in the labour force. The y-axis represents the log of income, while the x-axis distinguishes between females and males. The boxplots show that both groups have a similar range of income, with the male group exhibiting a wider spread and higher income values, as indicated by the larger box and more significant presence of outliers. The female group’s boxplot is more concentrated around the lower range of log income, with fewer outliers, suggesting that females generally earn less in comparison to males in this dataset. The median for females is lower, and the distribution is less variable than for males.

```{r}
#| label: fig-pred-sex
#| fig-cap: Log Income by Sex
#| echo: false
#| warning: false
#| message: false

ggplot(data, aes(x = Sex, y = log_income)) +
  geom_boxplot(fill = "lightblue", color = "darkblue") +
  xlab("Sex") +
  ylab("Log Income") +
  theme_minimal()

```
\pagebreak
In @fig-pred-married, the boxplot compares log-transformed income (Log Income) between married and not married individuals in the labour force. The y-axis represents the log of income, while the x-axis distinguishes between the two marital status groups. The boxplot for married individuals shows a higher median and wider distribution of income, with more outliers present, indicating a broader range of income levels and higher earnings overall. In contrast, the not married group has a lower median income, and the distribution is more tightly clustered around lower values, suggesting that individuals who are not married tend to have lower log income compared to their married counterparts.

```{r}
#| label: fig-pred-married
#| fig-cap: Log Income by Marital Status
#| echo: false
#| warning: false
#| message: false

ggplot(data, aes(x = Marital_status, y = log_income)) +
  geom_boxplot(fill = "lightblue", color = "darkblue") +
  xlab("Marital Status") +
  ylab("Log Income") +
  theme_minimal()

```
\pagebreak
In @fig-pred-educ, the boxplot compares log-transformed income (Log Income) across different education levels. The y-axis represents log income, while the x-axis shows various levels of education, ranging from less than 1st grade to professional/doctoral degrees. The plot reveals a clear positive trend: as education level increases, the median log income tends to rise, with individuals holding a Master's degree or Professional/Doctorate showing the highest median incomes. The spread of income also increases with higher education levels, particularly for individuals with Bachelor's degrees and beyond, where there are more outliers. This indicates that individuals with higher education levels generally have greater income, but the income distribution becomes more varied at the higher education levels.

```{r}
#| label: fig-pred-educ
#| fig-cap: Log Income by Educational Level
#| echo: false
#| warning: false
#| message: false

# Simplify the education levels
data$Education_simplified <- case_when(
  data$educ == -1 ~ "Less than 1st grade",
  data$educ == 1 ~ "1st-4th grade",
  data$educ == 2 ~ "5th-6th grade",
  data$educ == 3 ~ "7th-8th grade",
  data$educ == 4 ~ "9th grade",
  data$educ == 5 ~ "10th grade",
  data$educ == 6 ~ "11th grade",
  data$educ == 7 ~ "12th grade, no diploma",
  data$educ == 8 ~ "High school graduate",
  data$educ == 9 ~ "Some college",
  data$educ == 10 ~ "Associate degree (vocational)",
  data$educ == 11 ~ "Associate degree (academic)",
  data$educ == 12 ~ "Bachelor's degree",
  data$educ == 13 ~ "Master's degree",
  data$educ == 14 ~ "Professional/Doctorate"
)
# Reorder the 'Education_simplified' based on 'educ' numbers from small to large
data$Education_simplified <- factor(data$Education_simplified, 
                                    levels = c("Less than 1st grade", "1st-4th grade", "5th-6th grade", 
                                               "7th-8th grade", "9th grade", "10th grade", "11th grade", 
                                               "12th grade, no diploma", "High school graduate", "Some college", 
                                               "Associate degree (vocational)", "Associate degree (academic)", 
                                               "Bachelor's degree", "Master's degree", 
                                               "Professional/Doctorate"))

# Create the plot with simplified labels in correct order
ggplot(data, aes(x = Education_simplified, y = log_income)) +
  geom_boxplot(fill = "lightblue", color = "darkblue") +
  xlab("Education Level") +
  ylab("Log Income") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability

```
\pagebreak
In @fig-pred-lf, the boxplot compares log-transformed income (Log Income) between individuals who are working and those who are not working at all in the labour force. The y-axis represents log income, while the x-axis differentiates between the two groups. The plot shows that individuals working in some way generally have higher log incomes, with a wider spread and more outliers, particularly at the upper end. In contrast, those not working at all tend to have lower log incomes, with a narrower distribution and fewer outliers. The median for the "working in some way" group is higher, indicating that employment is associated with higher income. The presence of more outliers in the working group suggests greater income variability among those employed.

```{r}
#| label: fig-pred-lf
#| fig-cap: Log Income by Labour Force
#| echo: false
#| warning: false
#| message: false

ggplot(data, aes(x = Labor_force, y = log_income)) +
  geom_boxplot(fill = "lightblue", color = "darkblue") +
  xlab("Labour Force") +
  ylab("Log Income") +
  theme_minimal()

```
\pagebreak
In @fig-pred-worksta, the boxplot compares log-transformed income (Log Income) across four different work statuses: Not working, Retired/Disabled, Self-employed/Partnership, and Work for someone else. The y-axis represents log income, while the x-axis differentiates between the work status categories. Individuals who work for someone else tend to have the highest median log income, with a wider spread and more outliers, indicating greater income variability. Self-employed/partnership individuals also show higher median incomes but with a slightly smaller spread. Those who are retired/disabled have a narrower distribution, with a lower median income, and the not working group has the lowest median income, with a similar narrow distribution. The plot clearly demonstrates that employment, particularly in self-employment or working for someone else, is associated with higher log income compared to being retired, disabled, or not working.

```{r}
#| label: fig-pred-worksta
#| fig-cap: Log Income by Work Status
#| echo: false
#| warning: false
#| message: false

ggplot(data, aes(x = Work_status_category, y = log_income)) +
  geom_boxplot(fill = "lightblue", color = "darkblue") +
  xlab("Work Status") +
  ylab("Log Income") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```
\pagebreak
In @fig-pred-occup, the boxplot compares log-transformed income (Log Income) across four occupation categories: Managerial/Professional, Not working, Other (production workers/farmers), and Technical/Sales/Services. The y-axis represents log income, and the x-axis differentiates between the occupation groups. Individuals in the Managerial/Professional category have the highest median income, with a wider spread and many outliers, indicating significant income variability. The Technical/Sales/Services category also shows a higher median income, though the distribution is somewhat narrower. The Other (production workers/farmers) group has a lower median income, with a spread similar to that of the Not working category, which has the lowest median income. This suggests that higher-paying jobs are generally found in managerial/professional and technical/sales/services occupations, while individuals in non-working or lower-skill jobs tend to have lower income.

```{r}
#| label: fig-pred-occup
#| fig-cap: Log Income by Occupation Category
#| echo: false
#| warning: false
#| message: false

ggplot(data, aes(x = Occupation_Classification, y = log_income)) +
  geom_boxplot(fill = "lightblue", color = "darkblue") +
  xlab("Occupation Category") +
  ylab("Log Income") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```
\pagebreak

# Model{#sec-model}

The goal of our modelling strategy is twofold. Firstly, we aim to accurately predict the total income of individual in United States that participated in the survey of 2022. Secondly, we seek to evaluate the efficacy of different modeling approaches, from simple linear regression to Bayesian generalized linear model - to understand their predictive capabilities and assess the underlying relationships between variables. By comparing these models, we can determine which approach provides the most robust and reliable predictions, which considering the variability and potential uncertainty in this data. 

Here we briefly describe the Bayesian analysis model used to investigate. Background details and diagnostics are included in [@sec-modeldetails].

## Model set-up{#sec-model-setup}

Define $y_i$ as the Total Income for each individual participated in the survey in the United States at 2022. The predictors are categorical variables with different levels explained in @sec-data-. 

\begin{align*}
\log(y_i) &= \beta_0 
    + \beta_1 \cdot \text{Sex} 
    + \beta_2 \cdot \text{Education Level} 
    + \beta_3 \cdot \text{Marital Status} \\
&\quad + \beta_4 \cdot \text{Labor Force} 
    + \beta_5 \cdot \text{Work Status Category} 
    + \beta_6 \cdot \text{Occupation Classification} 
    + \epsilon, \\
\beta_0              &\sim \text{Normal}(10, 5), \\
\beta_1, \beta_2, \ldots, \beta_6 &\sim \text{Normal}(0, 2.5), \\
\epsilon             &\sim \text{Normal}(0, \delta^2), \\
\delta               &\sim \text{Exponential}(1).
\end{align*}

## Linear Model {#sec-model-lm}

We have started with the linear model for predicting Total Income based on the selected variables. The model set up is:

\begin{align*}
\hat{\text{income}} &= \beta_0 + \beta_1 \cdot \text{Sexmale} + \beta_2 \cdot \text{Education Level} + \beta_3 \cdot \text{Marital Status} + \beta_4 \cdot \text{Labor Force} \\
&\quad + \beta_5 \cdot \text{Work Status} + \beta_6 \cdot \text{Occupation Category}
\end{align*} 

```{r}
#| echo: false
#| warning: false
#| message: false
lm_model <- lm(formula = income ~ Sex + educ + married + lf + num_work + num_occu, data = data)
prediction_1<- predict(lm_model, data)
```

From @fig-lm, we can see that our linear model do not show a good accuracy for the Total Income variable. The graph shows presence of various outliers which causes the graph to be focused near the start of the range. From the red dotted line, we can see that it has show a slight predictive power but still weak in this graph. 

```{r}
#| label: fig-lm
#| fig-cap: Linear Model has limited predictive accuracy for Total Income
#| echo: false
#| warning: false
#| message: false

ggplot(data, aes(x = income, y = prediction_1)) +
  geom_point(color = 'blue') +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(title = "Actual vs Predicted Values",
       x = "Actual pct",
       y = "Predicted pct") +
  theme_minimal()

```


```{r}
#| label: tbl-lmresult
#| fig-cap: Table for Linear Model Results
#| echo: false
#| warning: false
#| message: false

# Table for Linear Model Results
table_description <- tibble(
  Variable = c("(Intercept)", "Sexmale", "educ", "married", "lf", "num_work", "num_occu", 
               "Residual Standard Error", "Multiple R-squared", "Adjusted R-squared", "F-statistic", "p-value"),
  Estimate = c(-348940, 668026, 227466, -1103395, 116525, 1892834, -1273936, 
               "1236000", "0.02223", "0.02198", "86.3", "< 2e-16"),
  Std_Error = c(870243, 263287, 33274, 234718, 353617, 144769, 123257, 
                "", "", "", "", ""),
  t_value = c(-0.401, 2.537, 6.836, -4.701, 0.330, 13.075, -10.336, 
              "", "", "", "", ""),
  Pr_gt_Abs_t = c(0.6884, 0.0112, 8.35e-12, 2.60e-06, 0.7418, "< 2e-16", "< 2e-16", 
                  "", "", "", "", ""),
  Significance = c("", "*", "***", "***", "", "***", "***", "", "", "", "", "")
)

# Draw the table 
kable(table_description, col.names = c("Variable", "Estimate", "Std. Error", "t-value", "Pr(>|t|)", "Significance")) |>
  kable_styling(latex_options = c("striped", "hold_position"))
```

\pagebreak
## Log Transformation of Linear Model {#sec-model-log}
```{r}
#| echo: false
#| warning: false
#| message: false
lm_model2 <- lm(log_income ~ Sex + educ + married + lf + num_work + num_occu, data = data)
prediction_2 <- predict(lm_model2, data)

```



```{r}
#| label: fig-loglm
#| fig-cap: Log Linear Model has better predictive accuracy for log Total Income
#| echo: false
#| warning: false
#| message: false

ggplot(data, aes(x = log_income, y = prediction_2)) +
  geom_point(color = 'blue') +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(title = "Actual vs Predicted Values",
       x = "Actual pct",
       y = "Predicted pct") +
  theme_minimal()
```

```{r}
#| label: tbl-loglmresult
#| fig-cap: Table for Log Linear Model Results
#| echo: false
#| warning: false
#| message: false

library(tibble)

# Create a tibble to store the model summary
table_description <- tibble(
  Variable = c("(Intercept)", "Sexmale", "educ", "married", "lf", "num_work", "num_occu", 
               "Residual Standard Error", "Multiple R-squared", "Adjusted R-squared", "F-statistic", "p-value"),
  Estimate = c(11.359653, 0.421812, 0.184963, -1.014780, -0.122501, 0.488869, -0.533513, 
               "1.273", "0.4379", "0.4378", "2957", "< 2.2e-16"),
  Std_Error = c(0.089632, 0.027118, 0.003427, 0.024175, 0.036421, 0.014911, 0.012695, 
                "", "", "", "", ""),
  t_value = c(126.736, 15.555, 53.970, -41.976, -3.363, 32.786, -42.025, 
              "", "", "", "", ""),
  Pr_gt_Abs_t = c("< 2e-16", "< 2e-16", "< 2e-16", "< 2e-16", "0.000771", "< 2e-16", "< 2e-16", 
                  "", "", "", "", ""),
  Significance = c("***", "***", "***", "***", "**", "***", "***", "", "", "", "", "")
)

kable(table_description, col.names = c("Variable", "Estimate", "Std. Error", "t-value", "Pr(>|t|)", "Significance")) |>
  kable_styling(latex_options = c("striped", "hold_position"))

```
\pagebreak

## Bayes Model {#sec-model-bayes}

```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| output: false
first_model <- stan_glm(
  formula = income ~ Sex + educ + married + lf + num_work + num_occu,
  data = data,
  family = gaussian(),
  prior = normal(location = 0, scale = 2.5, autoscale = TRUE),
  prior_intercept = normal(location = 500, scale = 100, autoscale = TRUE),
  prior_aux = exponential(rate = 1, autoscale = TRUE),
  seed = 853
)
prediction_3 <- predict(first_model, data)
```

```{r}
#| label: fig-bayes
#| fig-cap: Bayes Model has limited predictive accuracy for Total Income
#| echo: false
#| warning: false
#| message: false

ggplot(data, aes(x = income, y = prediction_3)) +
  geom_point(color = 'blue') +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(title = "Actual vs Predicted Values",
       x = "Actual pct",
       y = "Predicted pct") +
  theme_minimal()
```
```{r}
#| label: tbl-bayes
#| fig-cap: Table for Log Bayes Model Results
#| echo: false
#| warning: false
#| message: false

# Create a tibble for Estimates, Fit Diagnostics, and MCMC diagnostics
table_description <- tibble(
  Variable = c("(Intercept)", "Sexmale", "Education Level", "Marital Status", "Labor Force", "Work Status", "Occupation Category", "sigma",
               "mean_PPD", "mcse", "Rhat", "n_eff"),
  Mean = c(-330526.5, 665201.5, 227389.2, -1106314.8, 109757.1, 1893452.3, -1276316.9, 12358471.7, 
           1610015.7, 14336.4, 1.0, 1.0),
  SD = c(890630.7, 264380.1, 33021.8, 236391.0, 362967.1, 143077.3, 124229.1, 57949.8, 
         113810.9, 4097.4, 1.0, 1.0),
  `10%` = c(-1458852.6, 323626.9, 185831.1, -1406586.4, -354290.6, 1714955.2, -1438524.9, 12283774.9, 
            1461896.5, 467.9, 1.0, 1.0),
  `50%` = c(-334319.1, 663194.0, 226841.4, -1110600.3, 110247.3, 1891923.1, -1274086.5, 12358169.3, 
            1609532.0, 3636.0, 1.0, 1.0),
  `90%` = c(802368.2, 1003960.6, 270562.7, -803618.0, 571555.5, 2076167.0, -1116421.2, 12431827.0, 
            1755087.9, 571555.5, 1.0, 1.0),
  `mcse` = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, 1825.7, NA, NA),
  `Rhat` = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, 1.0, 1.0, 1.0),
  `n_eff` = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, 3886, 3859, 4163)
)

kable(table_description, col.names = c("Variable", "Mean", "SD", "10%", "50%", "90%", "mcse", "Rhat", "n_eff")) |>
  kable_styling(latex_options = c("striped", "hold_position"))
```

\pagebreak
## Bayes Model on Log Transformation {#sec-model-logbayes}

```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| output: false
second_model <- stan_glm(
  formula = log_income ~ Sex + educ + married + lf + num_work + num_occu,
  data = data,
  family = gaussian(),
  prior = normal(location = 0, scale = 2.5, autoscale = TRUE),
  prior_intercept = normal(location = 10, scale = 5, autoscale = TRUE),
  prior_aux = exponential(rate = 1, autoscale = TRUE),
  seed = 853
)
prediction_4 <- predict(second_model, data)
```

```{r}
#| label: fig-bayeslog
#| fig-cap: Bayes Model has better predictive accuracy for log Total Income
#| echo: false
#| warning: false
#| message: false

ggplot(data, aes(x = log_income, y = prediction_4)) +
  geom_point(color = 'blue') +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(title = "Actual vs Predicted Values",
       x = "Actual pct",
       y = "Predicted pct") +
  theme_minimal()
```

```{r}
#| label: tbl-logbayes
#| fig-cap: Table for Log Bayes Model Results
#| echo: false
#| warning: false
#| message: false

# Create a tibble to store the model summary
table_description <- tibble(
  Variable = c("(Intercept)", "Sexmale", "educ", "married", "lf", "num_work", "num_occu", "sigma", 
               "mean_PPD", "mcse", "Rhat", "n_eff"),
  Mean = c(11.4, 0.4, 0.2, -1.0, -0.1, 0.5, -0.5, 1.3, 11.8, 0.0, 1.0, 1.0),
  SD = c(0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0),
  `10%` = c(11.2, 0.4, 0.2, -1.0, -0.2, 0.5, -0.5, 1.3, 11.8, 0.0, 1.0, 1.0),
  `50%` = c(11.4, 0.4, 0.2, -1.0, -0.1, 0.5, -0.5, 1.3, 11.8, 0.0, 1.0, 1.0),
  `90%` = c(11.5, 0.5, 0.2, -1.0, -0.1, 0.5, -0.5, 1.3, 11.8, 0.0, 1.0, 1.0),
  mcse = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, 0.0, 0.0, 0.0),
  Rhat = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, 1.0, 1.0, 1.0),
  n_eff = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, 3813, 3759, 4603)
)

kable(table_description, col.names = c("Variable", "Mean", "SD", "10%", "50%", "90%", "mcse", "Rhat", "n_eff")) |>
  kable_styling(latex_options = c("striped", "hold_position"))

```

## Model justification{#sec-model-justification}

We run the model in R [@citeR] using the `rstanarm` package of @rstanarm. We use the default priors from `rstanarm`. The prior intercept have been set to $N(10,5)$ because the histogram @fig-hist indicates a mean near 10-12 with variation of 5. In addition, various predictors would have strong correlation with our outcome variable Total Income. By setting the prior intercept would capture well the additional knowledge compared to the default priors from `rstanarm` package. In overall, the results from the Bayes log model show a comprehensive analysis of the relationship between various predictors and the log-transformed income. The estimates for the coefficients, such as Sexmale (0.4), educ (0.2), and num_work (0.5), indicate that being male, having more education, and working more hours are all associated with higher log income. Married individuals, however, have a negative coefficient (-1.0), suggesting that being married is associated with a decrease in log income, although this relationship is modest. The model also shows a sigma value of 1.3, indicating the residual standard error, which measures the spread of the log income data around the model’s predictions. The mean PPD (11.8) suggests that the posterior predictive distribution for income is centered around this value, reinforcing the model's fit. The MCMC diagnostics (Rhat values of 1.0 for all parameters) confirm that the model has converged, with effective sample sizes (n_eff) indicating adequate sampling efficiency for each parameter. Lastly, the model appears to be well-specified, with clear relationships between income and the included variables, and good convergence as evidenced by the diagnostics.

# Results {#sec-results}

Our results are summarized in @tbl-logbayes.

The graph displays the posterior estimates of the model parameters in a plot, with each circle representing the point estimate of a parameter and its corresponding value plotted along the x-axis. Most of the estimates are clustered near zero, indicating relatively small effects on the outcome variable, except for sigma, which is the largest estimate, located at the far right of the graph. This suggests that the model accounts for a substantial amount of variability in the dependent variable. The plot visually emphasizes the magnitude of the coefficients, with some parameters having minimal effect, while sigma represents the level of uncertainty in the model’s predictions. The results from the graph suggest that the majority of the model's predictors, have relatively small point estimates, but these estimates are still substantial enough to be considered meaningful in the context of the model.
```{r}
#| echo: false
#| eval: true
#| warning: false
#| message: false

bayes_model <-readRDS(file = here::here("models/bayesian_model_2.rds"))
plot(bayes_model)
```

\pagebreak

The model summary table shows key results from the regression analysis, with estimates for both the coefficients and model diagnostics. The Intercept is 11.36, indicating a significant baseline value for the outcome variable when all predictors are zero, with a small standard error (0.09), suggesting high precision. Sexmale has a coefficient of 0.42, statistically significant with a small standard error of 0.03, meaning that being male is associated with an increase in the outcome variable. The Education Level estimate (0.18) suggests a positive but modest effect on the outcome, with the very small standard error (0.00), indicating a highly precise estimate. Marital Status and Labor Force have negative coefficients of -1.01 and -0.12, respectively, suggesting that being married or part of the labor force is associated with a decrease in the outcome. Occupation Category also shows a negative relationship (-0.53), indicating a significant reduction in the outcome with different occupation categories. The model’s R² of 0.438 indicates that about 44% of the variance in the dependent variable is explained by the model, while the Adjusted R² confirms this relationship is statistically valid, considering the number of predictors. Log.Lik. (-37811.782) and ELPD (-37819.9) are used for model comparison, with lower values indicating better model fit. The RMSE of 1.27 suggests the model predicts the outcome with reasonable accuracy, with a small average error in predictions.

```{r}
#| echo: false
#| eval: true
#| label: tbl-modelresults
#| tbl-cap: "Explanatory models of factors influencing income"
#| warning: false
#| message: false

#modelsummary::modelsummary(list("First model" = bayes_model),statistic = "mad",fmt = 2)

# Create a data frame for the model summary
model_summary <- data.frame(
  Variable = c("(Intercept)", "Sexmale", "Education Level", "Marital Status", "Labor Force", "Work Status", "Occupation Category",
               "Number of Observation", "R2", "R2 Adj.", "Log.Lik.", "ELPD", "ELPD s.e.", "LOOIC", 
               "LOOIC s.e.", "WAIC", "RMSE"),
  Estimate = c(11.36, 0.42, 0.18, -1.01, -0.12, 0.49, -0.53, 22778, 0.438, 0.438, -37811.782, 
               -37819.9, 179.7, 75639.7, 359.5, 75639.7, 1.27),
  Std.Error = c(0.09, 0.03, 0.00, 0.02, 0.04, 0.01, 0.01, "", "", "", "","", "", "", "","", "")
)
kable(model_summary, caption = "Model Summary Table")

```


\pagebreak

# Discussion{#sec-discussion}

## Summary of Findings {#sec-discussion-summary}

In this paper, we have developed a Bayes linear model predicting the Total Income of United States individuals. The model has provided a accurate prediction in terms of middle income group. However, it is still hard to predict high and low income group. With our analysis, we have found that 

## Second discussion point

Please don't use these as sub-heading labels - change them to be what your point actually is.

## Third discussion point

## Weaknesses and next steps

Weaknesses and next steps should also be included.

\newpage

\appendix

# Appendix {#Appendix}



# Model details {#sec-modeldetails}

## Posterior predictive check {#sec-modeldetails-poscheck}

In @fig-ppcheckandposteriorvsprior-1 we implement a posterior predictive check. The plot displays the posterior predictive check for a Bayesian model. The dark line represents the observed data (denoted as y), while the lighter, faded lines represent the posterior predictive distribution (denoted as y_rep), which shows the distribution of replicated data based on the model's parameters. From the graph, we can observe that the posterior predictive distribution (y_rep) closely follows the observed data (y) distribution, especially around the peak between values 10 and 14. The model seems to capture the central tendency and variability of the observed data, as evidenced by the overlap of the two distributions. This suggests that the model is performing well in terms of replicating the observed data. However, the wider spread of the y_rep lines at higher values suggests some variability or uncertainty in the model’s predictions, which is expected in Bayesian modeling.

In @fig-ppcheckandposteriorvsprior-2 we compare the posterior with the prior. The graph presents posterior and prior distributions for the parameters of a Bayesian model. On the left, the posterior distributions show the estimated values of each parameter based on the data and the model. The vertical lines represent the credible intervals for each parameter, and the dots represent the point estimates. On the right, the prior distributions are shown for the same parameters, illustrating what the model assumed about each parameter before observing the data. The prior distributions are much wider and more diffuse compared to the posterior distributions, indicating less certainty about the parameters before incorporating data. The priors for most parameters have a wide spread, with the prior for Intercept being particularly broad, suggesting high uncertainty. The difference between the prior and posterior for each parameter shows the influence of the data in updating the beliefs about these parameters. This plot provides a useful comparison to see how the data has informed the model's parameter estimates.

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| label: fig-ppcheckandposteriorvsprior
#| layout-ncol: 2
#| fig-cap: "Examining how the model fits, and is affected by, the data"
#| fig-subcap: ["Posterior prediction check", "Comparing the posterior with the prior"]

pp_check(bayes_model) +
  theme_classic() +
  theme(legend.position = "bottom")

posterior_vs_prior(bayes_model) +
  theme_minimal() +
  scale_color_brewer(palette = "Set1") +
  theme(legend.position = "bottom") +
  coord_flip()
```

## Diagnostics{#sec-modeldetails-diagnostics}

@fig-tracerhatplot-1 is a trace plot. The graph displays trace plots for multiple parameters in a Bayesian model, generated through Markov Chain Monte Carlo (MCMC) sampling. The plot contains four separate MCMC chains, each representing an independent sequence of parameter samples, with the x-axis indicating the number of iterations (from 1 to 1000) and the y-axis showing the values of the parameter estimates at each iteration.The trace plots for each parameter indicate good convergence across all chains, with the chains mixing well and stabilizing at similar values over time. The lack of systematic drift and the stable fluctuations around central values suggest that the model has reached stationarity, and the sampling process is not biased by the starting values. This indicates that the MCMC chains are successfully exploring the posterior distribution, and the parameter estimates are reliable. Overall, the graph suggests that the model's parameters are well-estimated, with the chains converging to a consistent distribution, making the results trustworthy for our models. 

@fig-tracerhatplot-2 is a Rhat plot. This plot represents the Gelman-Rubin diagnostic (denoted as R̂), which is used to assess convergence in a Bayesian analysis. The x-axis shows the values of R̂, which indicates whether the multiple MCMC chains have converged to the same distribution. Most of the points lie very close to 1.00, indicating that these parameters have likely converged, as the chains are sampling from the same posterior distribution. A few points may be slightly above 1.00, but all are still well below 1.1, suggesting that the model is close to convergence. The plot shows R̂ ≤ 1.05 for several parameters, which is generally considered acceptable for convergence, while any parameters with R̂ above 1.1 would indicate poor convergence, but this is not visible in the plot. The small variation in R̂ values suggests the model is performing well and the chains have likely mixed adequately.

```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| label: fig-tracerhatplot
#| fig-cap: "Checking the convergence of the MCMC algorithm"
#| fig-subcap: ["Trace plot", "Rhat plot"]
#| layout-ncol: 2

plot(bayes_model, "trace")

plot(bayes_model, "rhat")
```
\newpage
# References {#sec-references}


